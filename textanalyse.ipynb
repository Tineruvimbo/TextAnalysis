{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fe08b4-4e6d-4c4d-b524-26863606fe61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download ALL required NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('punkt_tab')  # This is the missing resource!\n",
    "except:\n",
    "    print(\"NLTK downloads completed or already present\")\n",
    "\n",
    "class ResearchPaperThemeExtractor:\n",
    "    def __init__(self):\n",
    "        self.papers_text = []\n",
    "        self.paper_titles = []\n",
    "        self.cleaned_texts = []\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Add academic-specific stop words\n",
    "        academic_stopwords = {\n",
    "            'paper', 'study', 'research', 'method', 'result', 'conclusion',\n",
    "            'introduction', 'abstract', 'section', 'figure', 'table',\n",
    "            'reference', 'citation', 'author', 'journal', 'proceedings',\n",
    "            'however', 'therefore', 'moreover', 'furthermore', 'namely',\n",
    "            'et', 'al', 'etc', 'ie', 'eg', 'cf', 'vol', 'pp', 'no'\n",
    "        }\n",
    "        self.stop_words.update(academic_stopwords)\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF file with better error handling\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \" \"\n",
    "                \n",
    "                # Check if we got meaningful text\n",
    "                if len(text.strip()) < 50:  # If very little text extracted\n",
    "                    print(f\"Warning: Very little text extracted from {os.path.basename(pdf_path)}\")\n",
    "                    return None\n",
    "                \n",
    "                return text.strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_papers(self, folder_path):\n",
    "        \"\"\"Load all PDF papers from a folder with better filtering\"\"\"\n",
    "        try:\n",
    "            all_files = os.listdir(folder_path)\n",
    "            pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
    "            \n",
    "            print(f\"Found {len(pdf_files)} PDF files in folder\")\n",
    "            \n",
    "            for pdf_file in pdf_files:\n",
    "                pdf_path = os.path.join(folder_path, pdf_file)\n",
    "                print(f\"Processing: {pdf_file}...\", end=\" \")\n",
    "                \n",
    "                text = self.extract_text_from_pdf(pdf_path)\n",
    "                if text and len(text) > 100:  # Only add if substantial text extracted\n",
    "                    self.papers_text.append(text)\n",
    "                    self.paper_titles.append(pdf_file)\n",
    "                    print(\"✓ Loaded successfully\")\n",
    "                else:\n",
    "                    print(\"✗ Failed to load (empty or corrupted)\")\n",
    "        \n",
    "            print(f\"\\nSuccessfully loaded: {len(self.papers_text)} out of {len(pdf_files)} papers\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading papers: {e}\")\n",
    "    \n",
    "    def simple_tokenize(self, text):\n",
    "        \"\"\"Simple tokenization as fallback if NLTK fails\"\"\"\n",
    "        # Basic tokenization without NLTK\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "        return words\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text with fallback tokenization\"\"\"\n",
    "        try:\n",
    "            # Remove citations and references\n",
    "            text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "            text = re.sub(r'\\(\\w+ et al\\.?,?\\s*\\d{4}\\)', '', text)\n",
    "            \n",
    "            # Remove special characters and digits, keep only letters\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            # Try NLTK tokenization first, fallback to simple tokenization\n",
    "            try:\n",
    "                tokens = word_tokenize(text.lower())\n",
    "            except:\n",
    "                tokens = self.simple_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords and short tokens, lemmatize\n",
    "            processed_tokens = []\n",
    "            for token in tokens:\n",
    "                if token not in self.stop_words and len(token) > 2:\n",
    "                    try:\n",
    "                        processed_tokens.append(self.lemmatizer.lemmatize(token))\n",
    "                    except:\n",
    "                        processed_tokens.append(token)  # Fallback to original token\n",
    "            \n",
    "            return ' '.join(processed_tokens)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def preprocess_all_papers(self):\n",
    "        \"\"\"Preprocess all loaded papers\"\"\"\n",
    "        print(\"Preprocessing papers...\")\n",
    "        self.cleaned_texts = []\n",
    "        for i, text in enumerate(self.papers_text):\n",
    "            cleaned = self.preprocess_text(text)\n",
    "            if cleaned and len(cleaned) > 10:  # Only keep if we have meaningful content\n",
    "                self.cleaned_texts.append(cleaned)\n",
    "            else:\n",
    "                print(f\"Paper {i+1} resulted in empty text after preprocessing\")\n",
    "        \n",
    "        print(f\"Preprocessing completed! {len(self.cleaned_texts)} papers ready for analysis.\")\n",
    "    \n",
    "    def extract_keywords_tfidf(self, top_n=20):\n",
    "        \"\"\"Extract keywords using TF-IDF\"\"\"\n",
    "        if not self.cleaned_texts:\n",
    "            print(\"No cleaned texts available for analysis\")\n",
    "            return [], None, None\n",
    "            \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.cleaned_texts)\n",
    "            \n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get overall important terms\n",
    "            overall_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "            top_indices = overall_tfidf.argsort()[-top_n:][::-1]\n",
    "            top_keywords = [(feature_names[i], overall_tfidf[i]) for i in top_indices]\n",
    "            \n",
    "            return top_keywords, tfidf_matrix, vectorizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in TF-IDF analysis: {e}\")\n",
    "            return [], None, None\n",
    "    \n",
    "    def topic_modeling_lda(self, num_topics=5):\n",
    "        \"\"\"Perform LDA topic modeling\"\"\"\n",
    "        if not self.cleaned_texts:\n",
    "            return [], None, None\n",
    "            \n",
    "        try:\n",
    "            # Create document-term matrix\n",
    "            vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "            doc_term_matrix = vectorizer.fit_transform(self.cleaned_texts)\n",
    "            \n",
    "            # Apply LDA\n",
    "            lda = LatentDirichletAllocation(\n",
    "                n_components=num_topics, \n",
    "                random_state=42,\n",
    "                max_iter=10\n",
    "            )\n",
    "            lda.fit(doc_term_matrix)\n",
    "            \n",
    "            # Extract topics\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            topics = []\n",
    "            \n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                top_features_ind = topic.argsort()[-10:][::-1]\n",
    "                top_features = [feature_names[i] for i in top_features_ind]\n",
    "                topics.append((f\"Topic {topic_idx + 1}\", top_features))\n",
    "            \n",
    "            return topics, lda, doc_term_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LDA analysis: {e}\")\n",
    "            return [], None, None\n",
    "    \n",
    "    def topic_modeling_nmf(self, num_topics=5):\n",
    "        \"\"\"Perform NMF topic modeling\"\"\"\n",
    "        if not self.cleaned_texts:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(self.cleaned_texts)\n",
    "            \n",
    "            nmf = NMF(n_components=num_topics, random_state=42)\n",
    "            nmf.fit(tfidf_matrix)\n",
    "            \n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            topics = []\n",
    "            \n",
    "            for topic_idx, topic in enumerate(nmf.components_):\n",
    "                top_features_ind = topic.argsort()[-10:][::-1]\n",
    "                top_features = [feature_names[i] for i in top_features_ind]\n",
    "                topics.append((f\"Topic {topic_idx + 1}\", top_features))\n",
    "            \n",
    "            return topics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in NMF analysis: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_word_cloud(self):\n",
    "        \"\"\"Generate word cloud from all papers\"\"\"\n",
    "        if not self.cleaned_texts:\n",
    "            print(\"No data available for word cloud\")\n",
    "            return\n",
    "            \n",
    "        all_text = ' '.join(self.cleaned_texts)\n",
    "        \n",
    "        if len(all_text.strip()) < 10:\n",
    "            print(\"Not enough text for word cloud\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                max_words=100\n",
    "            ).generate(all_text)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title('Word Cloud of Research Papers Themes')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating word cloud: {e}\")\n",
    "    \n",
    "    def plot_top_keywords(self, top_keywords):\n",
    "        \"\"\"Plot top keywords\"\"\"\n",
    "        if not top_keywords:\n",
    "            print(\"No keywords to plot\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            keywords, scores = zip(*top_keywords[:15])\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.barh(keywords, scores)\n",
    "            plt.xlabel('TF-IDF Score')\n",
    "            plt.title('Top 15 Keywords Across All Papers')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting keywords: {e}\")\n",
    "    \n",
    "    def analyze_themes(self):\n",
    "        \"\"\"Comprehensive theme analysis\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RESEARCH PAPER THEME ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if not self.papers_text:\n",
    "            print(\"No papers to analyze!\")\n",
    "            return None\n",
    "        \n",
    "        # Preprocess papers\n",
    "        self.preprocess_all_papers()\n",
    "        \n",
    "        if not self.cleaned_texts:\n",
    "            print(\"No papers could be processed for analysis!\")\n",
    "            return None\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Extract keywords\n",
    "        print(\"\\n1. TOP KEYWORDS ANALYSIS\")\n",
    "        print(\"-\" * 30)\n",
    "        top_keywords, tfidf_matrix, vectorizer = self.extract_keywords_tfidf()\n",
    "        if top_keywords:\n",
    "            for keyword, score in top_keywords[:15]:\n",
    "                print(f\"{keyword}: {score:.4f}\")\n",
    "            results['keywords'] = top_keywords\n",
    "        else:\n",
    "            print(\"No keywords extracted\")\n",
    "            results['keywords'] = []\n",
    "        \n",
    "        # Topic modeling with LDA\n",
    "        print(\"\\n2. LDA TOPIC MODELING\")\n",
    "        print(\"-\" * 30)\n",
    "        lda_topics, lda_model, doc_term_matrix = self.topic_modeling_lda()\n",
    "        if lda_topics:\n",
    "            for topic_name, keywords in lda_topics:\n",
    "                print(f\"{topic_name}: {', '.join(keywords[:8])}\")\n",
    "            results['lda_topics'] = lda_topics\n",
    "        else:\n",
    "            print(\"No LDA topics extracted\")\n",
    "            results['lda_topics'] = []\n",
    "        \n",
    "        # Topic modeling with NMF\n",
    "        print(\"\\n3. NMF TOPIC MODELING\")\n",
    "        print(\"-\" * 30)\n",
    "        nmf_topics = self.topic_modeling_nmf()\n",
    "        if nmf_topics:\n",
    "            for topic_name, keywords in nmf_topics:\n",
    "                print(f\"{topic_name}: {', '.join(keywords[:8])}\")\n",
    "            results['nmf_topics'] = nmf_topics\n",
    "        else:\n",
    "            print(\"No NMF topics extracted\")\n",
    "            results['nmf_topics'] = []\n",
    "        \n",
    "        # Generate visualizations\n",
    "        print(\"\\n4. GENERATING VISUALIZATIONS...\")\n",
    "        if top_keywords:\n",
    "            self.plot_top_keywords(top_keywords)\n",
    "        self.generate_word_cloud()\n",
    "        \n",
    "        return results\n",
    "\n",
    "# MAIN EXECUTION\n",
    "def main():\n",
    "    # Initialize the theme extractor\n",
    "    extractor = ResearchPaperThemeExtractor()\n",
    "    \n",
    "    # Use your path - FIXED with raw string\n",
    "    papers_folder = r\"C:\\Users\\mahac\\Desktop\\pdf_files\"\n",
    "    \n",
    "    print(f\"Looking for papers in: {papers_folder}\")\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(papers_folder):\n",
    "        print(f\"Error: Folder '{papers_folder}' does not exist!\")\n",
    "        print(\"Please check the path and try again.\")\n",
    "        return\n",
    "    \n",
    "    # Load papers\n",
    "    extractor.load_papers(papers_folder)\n",
    "    \n",
    "    # Perform analysis if papers were loaded\n",
    "    if extractor.papers_text:\n",
    "        results = extractor.analyze_themes()\n",
    "        \n",
    "        if results:\n",
    "            # Print summary\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ANALYSIS SUMMARY\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Total papers analyzed: {len(extractor.papers_text)}\")\n",
    "            print(f\"Papers successfully processed: {len(extractor.cleaned_texts)}\")\n",
    "            print(f\"Main themes identified: {len(results.get('lda_topics', []))}\")\n",
    "            print(f\"Top keywords found: {len(results.get('keywords', []))}\")\n",
    "    else:\n",
    "        print(\"No papers were loaded. Please check:\")\n",
    "        print(\"1. The folder contains PDF files\")\n",
    "        print(\"2. PDF files are not password protected\")\n",
    "        print(\"3. PDF files contain extractable text (not scanned images)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb6fcb-c5b1-4602-9872-271fbba8d8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
